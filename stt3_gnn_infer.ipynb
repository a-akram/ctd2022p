{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Inference after DNN Stage_\n",
    "\n",
    "**_Inference_** is done using callbacks defined in the **_LightningModules/GNN/Models/inference.py_**. The callbacks run during the **_test_step()_** _a.k.a_ model _**evalution**_.\n",
    "\n",
    "### How to run _Inference_?\n",
    "\n",
    "1. _`traintrack config/pipeline_quickstart.yaml`_. One can use `--inference` flag to run only the `test_step()` (Should work, but failed.)\n",
    "2. _`infer.ipynb`_ notebook runs the _pl.Trainer().test()_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, glob, yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import trackml.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['EXATRKX_DATA'] = os.path.abspath(os.curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LightningModules.GNN import InteractionGNN\n",
    "from LightningModules.GNN import GNNBuilder, GNNMetrics\n",
    "from LightningModules.GNN.Models.infer import GNNTelemetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## _Classifier Evaluation_\n",
    "\n",
    "Metrics to evaluate the GNN networks:\n",
    "\n",
    "- Accuracy/ACC = $TP+TN/TP+TN+FP+FN$\n",
    "- sensitivity, recall, hit rate, or true positive rate ($TPR = 1 - FNR$)\n",
    "- specificity, selectivity or true negative rate ($TNR = 1 - FPR$)\n",
    "- miss rate or false negative rate ($FNR = 1 - TPR$)\n",
    "- fall-out or false positive rate ($FPR = 1 - TNR$)\n",
    "- F1-score = $2 \\times (\\text{PPV} \\times \\text{TPR})/(\\text{PPV} + \\text{TPR})$\n",
    "- Efficiency/Recall/Sensitivity/Hit Rate: $TPR = TP/(TP+FN)$\n",
    "- Purity/Precision/Positive Predictive Value: $PPV = TP/(TP+FP$\n",
    "- AUC-ROC Curve $\\equiv$ FPR ($x-$axis) v.s. TPR ($y-$axis) plot\n",
    "- AUC-PRC Curve $\\equiv$ TPR ($x-$axis) v.s. PPV ($y-$axis) plot\n",
    "\n",
    "\n",
    "Use `tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()` to directly access TN, FP, FN and TP using Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Test Dataset_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test Dataset from GNNBuilder\n",
    "inputdir=\"run/dnn_processed/test\"\n",
    "all_events = sorted(glob.glob(os.path.join(inputdir, \"*\")))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load Single Event\n",
    "test_event = torch.load(all_events[0], map_location=device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_event"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loaded_events = []\n",
    "for event in tqdm(all_events):\n",
    "    loaded_events.append(torch.load(event, map_location=device))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# PyG DataLoader\n",
    "test_dataloader = DataLoader(loaded_events, batch_size=1, num_workers=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Fetch One Batch\n",
    "sampled_data = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Print One Batch\n",
    "print(sampled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfvKUCvjUlQA"
   },
   "source": [
    "### _Load Checkpoint_\n",
    "\n",
    "Lightning automatically saves a checkpoint for you in your current working directory, with the state of your last training epoch. We have checkpoint stored after training is finished.\n",
    "\n",
    "```\n",
    "# load a LightningModule along with its weights & hyperparameters from a checkpoint\n",
    "model = MyLightningModule.load_from_checkpoint(\"/path/to/checkpoint.ckpt\")\n",
    "print(model.input_dir)\n",
    "```\n",
    "\n",
    "Note that we have saved our hyperparameters when our **LightningModule** was initialized i.e. `self.save_hyperparameters(hparams)`\n",
    "\n",
    "```\n",
    "# hyperparameters are saved to the “hyper_parameters” key in the checkpoint, to access them\n",
    "checkpoint = torch.load(path/to/checkpoint, map_location=device)\n",
    "print(checkpoint[\"hyper_parameters\"])\n",
    "```\n",
    "\n",
    "One can also initialize the model with different hyperparameters (if they are saved).\n",
    "\n",
    "\n",
    "For more details, consult [Lighting Checkpointing](https://pytorch-lightning.readthedocs.io/en/stable/common/checkpointing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Get Checkpoint Hparams_\n",
    "\n",
    "- Either from the configs folder \n",
    "- Or extract it from the checkpoint, favoured if model is trained and evaluated on two different machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processing config file (trusted source)\n",
    "config = None\n",
    "config_file = os.path.join(os.curdir, 'LightningModules/DNN/configs/train_alldata_DNN.yaml')\n",
    "with open(config_file) as f:\n",
    "    try:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader) # equiv: yaml.full_load(f)\n",
    "    except yaml.YAMLError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model Checkpoint\n",
    "ckpnt_path = \"run_all/lightning_models/lightning_checkpoints/GNNStudy/a58b2mlx/checkpoints/last.ckpt\"\n",
    "# ckpnt_path = \"run_all/lightning_models/lightning_checkpoints/HypGNN/uibb0ir9/checkpoints/last.ckpt\"\n",
    "\n",
    "checkpoint = torch.load(ckpnt_path, map_location=device)\n",
    "config = checkpoint[\"hyper_parameters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint[\"hyper_parameters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Hyperparameters\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Can Modify Hyperparameters\n",
    "config[\"checkpoint_path\"] = ckpnt_path\n",
    "config[\"input_dir\"] = \"run_quick/feature_store\"\n",
    "config[\"output_dir\"] = \"run_quick/gnn_processed\"\n",
    "config[\"artifact_library\"] = \"lightning_models/lightning_checkpoints\"\n",
    "config[\"train_split\"] = [0, 0, 10000]\n",
    "config[\"map_location\"] = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Hyperparameters (Modified)\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Get Checkpoint Model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init EdgeClassifier with New Config\n",
    "model = InteractionGNN(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint with New Config (It will Provide Path and Other Parameters, Most will be Overwritten)\n",
    "model = model.load_from_checkpoint(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _(1) - Inference: Callbacks_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Test with LightingModule_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning Trainer\n",
    "trainer = pl.Trainer(callbacks=[GNNBuilder()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TestStep\n",
    "# trainer.test(model=model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Test with LightningDataModule_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Predict import SttDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LightningDataModule\n",
    "# dm = SttDataModule(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm.setup(stage='test')\n",
    "# test_dataloaders = dm.test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TestStep with LightningDataModule\n",
    "# trainer.test(model=model, dataloaders=None, ckpt_path=None, verbose=True, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _(2) - Inference: Manual_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Predict import eval_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _How to get data using LightningModuel?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run setup() for datasets\n",
    "# model.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Directly Get Test Dataset\n",
    "# testset = model.testset\n",
    "\n",
    "# Get singel Batch\n",
    "# batch = testset[0]\n",
    "\n",
    "# OR, loop over\n",
    "# for index, batch in enumerate(testset):\n",
    "# for batch in testset:\n",
    "#    print(index, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Directly Get Test Dataloader\n",
    "# test_dataloader = model.test_dataloader()\n",
    "\n",
    "# Get singel Batch\n",
    "# batch = next(iter(test_dataloader))\n",
    "\n",
    "# OR, loop over\n",
    "# for batch_idx, batch in enumerate(test_dataloader):\n",
    "# for batch in test_dataloader:\n",
    "#    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Run `eval_model()` on `test_dataloader()`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get testset or test_dataloader\n",
    "# testset = model.testset\n",
    "# test_dataloader = model.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model, returns torch tensors\n",
    "# scores, truths = eval_model(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _(3) - Inference: BNNBuilder_\n",
    "\n",
    "_If **GNNBuilder** callback has been run during training, just load data from `dnn_processed/test` and extract `scores` and `y_pid ~ truth` and simply run the following metrics_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all files\n",
    "inputdir = \"run_all/gnn_processed/test\"\n",
    "gnn_files = sorted(glob.glob(os.path.join(inputdir, \"*\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Load all `truth` and `scores` from the `testset` from the `DNN` stage_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresl, truthsl = [], []\n",
    "\n",
    "for e in range(len(gnn_files)):\n",
    "    \n",
    "    # logging\n",
    "    if e !=0 and e%1000==0:\n",
    "        print(\"Processed Batches: \", e)\n",
    "    \n",
    "    gnn_data = torch.load(gnn_files[e], map_location=device)\n",
    "    \n",
    "    truth = gnn_data.y_pid\n",
    "    score = gnn_data.scores\n",
    "    score = score[:truth.size(0)]\n",
    "    \n",
    "    # append each batch\n",
    "    scoresl.append(score)\n",
    "    truthsl.append(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.cat(scoresl)\n",
    "truths = torch.cat(truthsl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Evaluation Metrics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metric_utils import compute_metrics, plot_metrics\n",
    "from src.metric_utils import plot_roc, plot_prc, plot_prc_thr, plot_epc, plot_epc_cut, plot_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scores and truths as .npy files\n",
    "# np.save(\"gnn_scores.npy\", scores.numpy())\n",
    "# np.save(\"gnn_truths.npy\", truths.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch to numpy\n",
    "scores = scores.numpy()\n",
    "truths = truths.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_metrics(scores,truths,threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curves\n",
    "# metrics.prc_precision, metrics.prc_recall, metrics.prc_thresh\n",
    "# metrics.roc_tpr, metrics.roc_fpr, metrics.roc_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _(a) - Plot Metrics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outname = \"gnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(scores,truths, metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "# plot_roc(metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR Curve\n",
    "# plot_prc(metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built from PRC Curve\n",
    "# plot_prc_thr(metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EP Curve from ROC\n",
    "# plot_epc(metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built from ROC Curve\n",
    "# plot_epc_cut(metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model output: True and False\n",
    "# plot_output(scores, truths, threshold=0.5, name=outname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _(b) - S/B Suppression_\n",
    "\n",
    "Background rejection rate (1/FPR) is given as $1/\\epsilon_{bkg}$ where $\\epsilon_{bkg}$ is the fraction of fake edges that pass the classification requirement. Signal efficiency (TPR ~ Recall) ($\\epsilon_{sig}$) is defined as the number of true edges above a given classification score cut over the total number of true edges. What we have?\n",
    "\n",
    "- Signal Efficiency = $\\epsilon_{sig}$ = TPR ~ Recall \n",
    "- Background Rejection = $1 - \\epsilon_{bkg}$ ???\n",
    "- Background Rejection Rate = $1/\\epsilon_{bkg}$ = 1/FPR\n",
    "\n",
    "\n",
    "First apply a edge score cut to binarized the `scores`, we will call it `preds`. The count number of false or true edges that pass this cut. Then calculated background rejection rate and signal efficiency. For making a plot one can do calculations in batch by batch mode on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = metrics.roc_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = metrics.roc_fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_rejection = 1/fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut off eff < 0.2 or 0.5\n",
    "sig_mask = sig > 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,6))\n",
    "ax.plot(sig[sig_mask], bkg_rejection[sig_mask], color=\"blue\")\n",
    "\n",
    "# Axes Params\n",
    "ax.set_xlabel(\"Signal Efficiency ($\\epsilon_{sig}$)\", fontsize=16)\n",
    "ax.set_ylabel(\"Background Rejection ($1/\\epsilon_{bkg}$)\", fontsize=16)\n",
    "ax.set_yscale('log')\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=12)\n",
    "ax.grid(True)\n",
    "\n",
    "# Figure Params\n",
    "fig.tight_layout()\n",
    "fig.savefig(outname+\"_SB.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Plot Test Event_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "example_data = testset[0]\n",
    "r, phi, ir = example_data.x.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x, y = r * np.cos(phi * np.pi), r * np.sin(phi * np.pi)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(x, y, s=2)\n",
    "plt.title(\"Azimuthal View of Detector\", fontsize=24), plt.xlabel(\n",
    "    \"x\", fontsize=18\n",
    "), plt.ylabel(\"y\", fontsize=18)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "e = example_data.edge_index\n",
    "pid = example_data.pid\n",
    "true_edges = pid[e[0]] == pid[e[1]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "# plt.plot(x[e[:, ~true_edges]], y[e[:, ~true_edges]], c=\"r\")\n",
    "plt.plot(x[e[:, true_edges]], y[e[:, true_edges]], c=\"k\")\n",
    "plt.scatter(x, y, s=5)\n",
    "plt.title(\"Azimuthal View of Detector\", fontsize=24), plt.xlabel(\n",
    "    \"x\", fontsize=18\n",
    "), plt.ylabel(\"y\", fontsize=18)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(x[e[:, (~true_edges)][:, 0:-1:5]], y[e[:, (~true_edges)][:, 0:-1:5]], c=\"r\")\n",
    "plt.scatter(x, y, s=5)\n",
    "plt.title(\"Azimuthal View of Detector\", fontsize=24), plt.xlabel(\n",
    "    \"x\", fontsize=18\n",
    "), plt.ylabel(\"y\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _TensorBoard Logger_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load TensorBoard notebook extension\n",
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# %tensorboard --logdir=run_all/lightning_models/lightning_checkpoints/DNNStudy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
