{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Evaluation Metrics_\n",
    "\n",
    "_If **GNNBuilder** callback has been run during training, just load data from `dnn_processed/test` and extract `scores` and `y_pid ~ truth` and simply run the following metrics_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, glob, yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import trackml.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append parent dir\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation Definitions\n",
    "\n",
    "Metrics to evaluate the GNN networks:\n",
    "\n",
    "- Accuracy/ACC = $TP+TN/TP+TN+FP+FN$\n",
    "- sensitivity, recall, hit rate, or true positive rate ($TPR = 1 - FNR$)\n",
    "- specificity, selectivity or true negative rate ($TNR = 1 - FPR$)\n",
    "- miss rate or false negative rate ($FNR = 1 - TPR$)\n",
    "- fall-out or false positive rate ($FPR = 1 - TNR$)\n",
    "- F1-score = $2 \\times (\\text{PPV} \\times \\text{TPR})/(\\text{PPV} + \\text{TPR})$\n",
    "- Efficiency/Recall/Sensitivity/Hit Rate: $TPR = TP/(TP+FN)$\n",
    "- Purity/Precision/Positive Predictive Value: $PPV = TP/(TP+FP$\n",
    "- AUC-ROC Curve $\\equiv$ FPR ($x-$axis) v.s. TPR ($y-$axis) plot\n",
    "- AUC-PRC Curve $\\equiv$ TPR ($x-$axis) v.s. PPV ($y-$axis) plot\n",
    "\n",
    "\n",
    "Use _`tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()`_ to directly access TN, FP, FN and TP using Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Classifier Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all files\n",
    "# inputdir = \"run_all/gnn_processed/test\"\n",
    "# inputdir = \"run_all/dnn_processed_bn/test\"\n",
    "# inputdir = \"run_all/dnn_processed_ln/test\"\n",
    "\n",
    "# HypGNN (FWP + Filtering)\n",
    "# inputdir = \"run_all/fwp_gnn_processed_nf/pred\"\n",
    "\n",
    "# HypGNN (FWP + No Filtering)\n",
    "inputdir = \"run_all/fwp_gnn_processed/pred\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = sorted(glob.glob(os.path.join(inputdir, \"*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test a event\n",
    "torch.load(test_files[0], map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Append Scores and Truths_\n",
    "- _Load all `truth` and `scores` from the `testset` from the `DNN` stage_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresl, truthsl = [], []\n",
    "\n",
    "for e in range(len(test_files)):\n",
    "    \n",
    "    # read test events e.g. gnn_processed/test\n",
    "    data = torch.load(test_files[e], map_location=device)\n",
    "    \n",
    "    # get truths and scores\n",
    "    truth = data.y_pid\n",
    "    score = data.scores\n",
    "    score = score[:truth.size(0)]\n",
    "    \n",
    "    # logging\n",
    "    if e !=0 and (e)%1000==0:\n",
    "        print(\"Processed Batches: \", e)\n",
    "        \n",
    "    # append each batch\n",
    "    scoresl.append(score)\n",
    "    truthsl.append(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.cat(scoresl)\n",
    "truths = torch.cat(truthsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scores and truths as .npy files\n",
    "# np.save(\"scores.npy\", scores.numpy())\n",
    "# np.save(\"truths.npy\", truths.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Compute Metrics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metric_utils import compute_metrics, plot_metrics\n",
    "from src.metric_utils import plot_roc, plot_prc, plot_prc_thr, plot_epc, plot_epc_cut, plot_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch to numpy\n",
    "scores = scores.numpy()\n",
    "truths = truths.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_metrics(scores,truths,threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _(a) - Plot Metrics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outname = \"fwp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metrics(scores,truths, metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "# plot_roc(metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR Curve\n",
    "# plot_prc(metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built from PRC Curve\n",
    "# plot_prc_thr(metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EP Curve from ROC\n",
    "plot_epc(metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built from ROC Curve\n",
    "plot_epc_cut(metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model output: True and False\n",
    "plot_output(scores, truths, threshold=0.9, name=outname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _filter the **bumpy** region_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, targets = scores, truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a mask around this region\n",
    "mask = np.where((preds < 0.8) & (preds > 0.6))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter, preds and targets\n",
    "preds = preds[mask]\n",
    "labels = targets[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure & Axes\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
    "\n",
    "# Ploting\n",
    "binning = dict(bins=25, range=(0, 1), histtype='step', log=True)\n",
    "axs.hist(preds[labels == True], label='Real', **binning)  # True Class\n",
    "axs.hist(preds[labels == False], label='Fake', **binning)  # False Class\n",
    "\n",
    "# Axes Params\n",
    "# axs.set_title(\"Classifier Output\", fontsize=15)\n",
    "axs.set_xlabel('Model Output', size=20)\n",
    "axs.set_ylabel('Counts', size=20)\n",
    "axs.tick_params(axis='both', which='major', labelsize=12)\n",
    "axs.tick_params(axis='both', which='minor', labelsize=12)\n",
    "# axs.set_ylim(ymin=.005)\n",
    "axs.legend(fontsize=14, loc='upper center')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _(b) - S/B Suppression_\n",
    "\n",
    "Background rejection rate (1/FPR) is given as $1/\\epsilon_{bkg}$ where $\\epsilon_{bkg}$ is the fraction of fake edges that pass the classification requirement. Signal efficiency (TPR ~ Recall) ($\\epsilon_{sig}$) is defined as the number of true edges above a given classification score cut over the total number of true edges. What we have?\n",
    "\n",
    "- Signal Efficiency = $\\epsilon_{sig}$ = TPR ~ Recall \n",
    "- Background Rejection = $1 - \\epsilon_{bkg}$ ???\n",
    "- Background Rejection Rate = $1/\\epsilon_{bkg}$ = 1/FPR\n",
    "\n",
    "\n",
    "First apply a edge score cut to binarized the `scores`, we will call it `preds`. The count number of false or true edges that pass this cut. Then calculated background rejection rate and signal efficiency. For making a plot one can do calculations in batch by batch mode on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = metrics.roc_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_rejection = 1/metrics.roc_fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut off eff < 0.2 or 0.5\n",
    "sig_mask = sig > 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,6))\n",
    "ax.plot(sig[sig_mask], bkg_rejection[sig_mask], label=\"Interaction GNN\", color=\"blue\")\n",
    "\n",
    "# Axes Params\n",
    "ax.set_xlabel(\"Signal Efficiency\", fontsize=16)\n",
    "ax.set_ylabel(\"Background Rejection Rate\", fontsize=16)\n",
    "ax.set_yscale('log')\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=12)\n",
    "ax.grid(True)\n",
    "ax.legend(fontsize=14, loc='upper right')\n",
    "    \n",
    "# Figure Params\n",
    "fig.tight_layout()\n",
    "# fig.savefig(outname+\"_SB.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _(c) - Visualize Model Output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.drawing import detector_layout\n",
    "from src.utils_math import polar_to_cartesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph\n",
    "graph = torch.load(test_files[e], map_location=device)\n",
    "\n",
    "# get truths and scores\n",
    "truth = graph.y_pid\n",
    "scores = graph.scores[:truth.size(0)]\n",
    "edges = graph.edge_index\n",
    "eid = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth.shape, scores.shape, edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, labels = score.numpy(), truth.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract hit information\n",
    "r, phi, ir = graph.x.T\n",
    "ir = ir.detach().numpy()*100\n",
    "x, y = polar_to_cartesian(r.detach().numpy(), phi.detach().numpy())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# plot true event\n",
    "fig, ax = detector_layout(figsize=(10,10))\n",
    "pids = np.unique(graph.pid)\n",
    "\n",
    "# True Event\n",
    "for pid in pids:\n",
    "    idx = graph.pid == pid\n",
    "    ax.plot(x[idx], y[idx], '-', linewidth=1.5)\n",
    "    ax.scatter(x[idx], y[idx], label='particle_id: {}'.format(int(pid)))\n",
    "\n",
    "ax.set_title('Azimuthal View of STT, EventID # {}'.format(eid))\n",
    "ax.legend(fontsize=10, loc='best')\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"true_track.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sample_xy(hits, edges, preds, labels, cut=0.5, figsize=(16, 16)):\n",
    "    \"\"\"\"Draw Sample with True and False Edges\"\"\"\n",
    "    \n",
    "    # coordinate transformation\n",
    "    r, phi, ir = hits.T\n",
    "    x, y = polar_to_cartesian(r, phi)\n",
    "    \n",
    "    # detector layout\n",
    "    fig, ax = detector_layout(figsize=figsize)\n",
    "    \n",
    "    # Draw the segments\n",
    "    for j in range(labels.shape[0]):\n",
    "        \n",
    "        ptx1 = x[edges[0,j]]\n",
    "        ptx2 = x[edges[1,j]]\n",
    "        pty1 = y[edges[0,j]]\n",
    "        pty2 = y[edges[1,j]]\n",
    "        \n",
    "        # False Negatives\n",
    "        if preds[j] < cut and labels[j] > cut:\n",
    "            # ax.plot([x[edges[0,j]], x[edges[1,j]]], [y[edges[0,j]], y[edges[1,j]]], '--', c='b')\n",
    "            ax.plot([ptx1, ptx2], [pty1, pty2], '--', color='b', lw=1.5, alpha=0.9)\n",
    "\n",
    "        # False Positives\n",
    "        if preds[j] > cut and labels[j] < cut:\n",
    "            # ax.plot([x[edges[0,j]], x[edges[1,j]]], [y[edges[0,j]], y[edges[1,j]]], '-', c='r', alpha=preds[j])\n",
    "            ax.plot([ptx1, ptx2], [pty1, pty2], '-', color='r', lw=1.5, alpha=0.15)\n",
    "\n",
    "        # True Positives\n",
    "        if preds[j] > cut and labels[j] > cut:\n",
    "            # ax.plot([x[edges[0,j]], x[edges[1,j]]], [y[edges[0,j]], y[edges[1,j]]], '-', c='k', alpha=preds[j])\n",
    "            ax.plot([ptx1, ptx2], [pty1, pty2], '-', color='k', lw=1.5, alpha=0.9)\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_sample_xy(graph.x.detach().numpy(), edges, preds, labels, cut=0.9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
