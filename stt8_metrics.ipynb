{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Evaluation Metrics_\n",
    "\n",
    "_If **GNNBuilder** callback has been run during training, just load data from `dnn_processed/test` and extract `scores` and `y_pid ~ truth` and simply run the following metrics_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, glob, yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import trackml.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append parent dir\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metric_utils import compute_metrics, plot_metrics\n",
    "from src.metric_utils import plot_roc, plot_prc, plot_prc_thr, plot_epc, plot_epc_cut, plot_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _Evaluation Definitions_\n",
    "\n",
    "Metrics to evaluate the GNN networks:\n",
    "\n",
    "- Accuracy/ACC = $TP+TN/TP+TN+FP+FN$\n",
    "- sensitivity, recall, hit rate, or true positive rate ($TPR = 1 - FNR$)\n",
    "- specificity, selectivity or true negative rate ($TNR = 1 - FPR$)\n",
    "- miss rate or false negative rate ($FNR = 1 - TPR$)\n",
    "- fall-out or false positive rate ($FPR = 1 - TNR$)\n",
    "- F1-score = $2 \\times (\\text{PPV} \\times \\text{TPR})/(\\text{PPV} + \\text{TPR})$\n",
    "- Efficiency/Recall/Sensitivity/Hit Rate: $TPR = TP/(TP+FN)$\n",
    "- Purity/Precision/Positive Predictive Value: $PPV = TP/(TP+FP$\n",
    "- AUC-ROC Curve $\\equiv$ FPR ($x-$axis) v.s. TPR ($y-$axis) plot\n",
    "- AUC-PRC Curve $\\equiv$ TPR ($x-$axis) v.s. PPV ($y-$axis) plot\n",
    "\n",
    "\n",
    "Use _`tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()`_ to directly access TN, FP, FN and TP using Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _Classifier Evaluation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all files\n",
    "# inputdir = \"run_all/gnn_processed/test\"\n",
    "# inputdir = \"run_all/dnn_processed_bn/test\"\n",
    "# inputdir = \"run_all/dnn_processed_ln/test\"\n",
    "\n",
    "# HypGNN (FWP + Filtering)\n",
    "# inputdir = \"run_all/fwp_gnn_processed_nf/pred\"\n",
    "\n",
    "# HypGNN (FWP + No Filtering)\n",
    "inputdir = \"run_all/fwp_gnn_processed/pred\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = sorted(glob.glob(os.path.join(inputdir, \"*\")))\n",
    "print(\"Number of Files: \", len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test a event\n",
    "data = torch.load(test_files[0], map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Append Scores and Truths_\n",
    "- _Load all `truth` and `scores` from the `testset` from the `DNN` stage_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresl, truthsl = [], []\n",
    "\n",
    "for e in range(len(test_files)):\n",
    "\n",
    "    # read test events e.g. gnn_processed/test\n",
    "    graph = torch.load(test_files[e], map_location=device)\n",
    "    \n",
    "    # get truths and scores\n",
    "    truth = graph.y_pid\n",
    "    score = graph.scores\n",
    "    score = score[:truth.size(0)]\n",
    "\n",
    "    # logging\n",
    "    if e !=0 and (e)%1000==0:\n",
    "        print(\"Processed Batches: \", e)\n",
    "        \n",
    "    # append each batch\n",
    "    truthsl.append(truth)\n",
    "    scoresl.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.cat(scoresl)\n",
    "truths = torch.cat(truthsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch to numpy\n",
    "scores = scores.numpy()\n",
    "truths = truths.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scores and truths as .npy files\n",
    "# np.save(\"scores.npy\", scores.numpy())\n",
    "# np.save(\"truths.npy\", truths.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Compute Metrics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_metrics(scores,truths,threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:.4f},{:.4f},{:.4f},{:.4f}\".format(metrics.accuracy, metrics.precision, metrics.recall, metrics.f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _(a) - Plot Metrics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outname = \"fwp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metrics(scores,truths, metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "# plot_roc(metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR Curve\n",
    "# plot_prc(metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built from PRC Curve\n",
    "# plot_prc_thr(metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EP Curve from ROC\n",
    "plot_epc(metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built from ROC Curve\n",
    "plot_epc_cut(metrics, name=outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model output: True and False\n",
    "plot_output(scores, truths, threshold=0.9, name=outname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _(b) - S/B Suppression_\n",
    "\n",
    "Background rejection rate (1/FPR) is given as $1/\\epsilon_{bkg}$ where $\\epsilon_{bkg}$ is the fraction of fake edges that pass the classification requirement. Signal efficiency (TPR ~ Recall) ($\\epsilon_{sig}$) is defined as the number of true edges above a given classification score cut over the total number of true edges. What we have?\n",
    "\n",
    "- Signal Efficiency = $\\epsilon_{sig}$ = TPR ~ Recall \n",
    "- Background Rejection = $1 - \\epsilon_{bkg}$ ???\n",
    "- Background Rejection Rate = $1/\\epsilon_{bkg}$ = 1/FPR\n",
    "\n",
    "\n",
    "First apply a edge score cut to binarized the `scores`, we will call it `preds`. The count number of false or true edges that pass this cut. Then calculated background rejection rate and signal efficiency. For making a plot one can do calculations in batch by batch mode on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics with Threshold\n",
    "metrics = compute_metrics(scores,truths,threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _recall/tpr and fpr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, targets = scores, truths\n",
    "y_pred, y_true = (preds > threshold), (targets > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "# Find Recal/TPR and FPR\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal, bkg, bkg rejection\n",
    "tpr, fpr, (1/fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _signal vs background rejection rate_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = metrics.roc_tpr\n",
    "bkg_rejection = 1/metrics.roc_fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut off eff < 0.2 or 0.5\n",
    "sig_mask = sig > 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,6))\n",
    "ax.plot(sig[sig_mask], bkg_rejection[sig_mask], label=\"Interaction GNN\", color=\"blue\")\n",
    "ax.plot(tpr, 1/fpr, marker=\"o\", markersize=10, markeredgecolor=\"k\", markerfacecolor=\"k\", label=\"Edge Score = 0.5\", color=\"k\")\n",
    "\n",
    "# Axes Params\n",
    "ax.set_xlabel(\"Signal Efficiency\", fontsize=16)\n",
    "ax.set_ylabel(\"Background Rejection\", fontsize=16)\n",
    "ax.set_yscale('log')\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=12)\n",
    "ax.grid(True)\n",
    "ax.legend(fontsize=14, loc='upper right')\n",
    "    \n",
    "# Figure Params\n",
    "fig.tight_layout()\n",
    "fig.savefig(outname+\"_SB.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _(c) - Visualize Model Output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.drawing import detector_layout\n",
    "from src.utils_math import polar_to_cartesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = filter_files[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph\n",
    "graph = torch.load(test_files[e], map_location=device)\n",
    "\n",
    "# get truths and scores\n",
    "truth = graph.y_pid\n",
    "scores = graph.scores[:truth.size(0)]\n",
    "edges = graph.edge_index\n",
    "eid = int(graph.event_file[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth.shape, scores.shape, edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, labels = scores.numpy(), truth.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sample_xy(graph, cut=0.5, figsize=(15,15)):\n",
    "    \"\"\"\"Draw Sample with True and False Edges\"\"\"\n",
    "    \n",
    "    # coordinate transformation\n",
    "    x = graph.x.detach().numpy()\n",
    "    r, phi, ir = x.T\n",
    "    x, y = polar_to_cartesian(r, phi)\n",
    "    \n",
    "    \n",
    "    truth = graph.y_pid\n",
    "    scores = graph.scores[:truth.size(0)]\n",
    "    edges = graph.edge_index\n",
    "    preds, labels = scores.numpy(), truth.numpy()\n",
    "    \n",
    "    \n",
    "    # detector layout\n",
    "    fig, ax = detector_layout(figsize=figsize)\n",
    "    \n",
    "    # True Event\n",
    "    pids = np.unique(graph.pid)    \n",
    "    for pid in pids:\n",
    "        idx = graph.pid == pid\n",
    "        ax.plot(x[idx], y[idx], 'k-', linewidth=1.5)\n",
    "        ax.scatter(x[idx], y[idx], label='particle_id: {}'.format(int(pid)))\n",
    "    \n",
    "    \n",
    "    # Draw the segments\n",
    "    for j in range(labels.shape[0]):\n",
    "        \n",
    "        ptx1 = x[edges[0,j]]\n",
    "        ptx2 = x[edges[1,j]]\n",
    "        pty1 = y[edges[0,j]]\n",
    "        pty2 = y[edges[1,j]]\n",
    "        \n",
    "        # False Negatives\n",
    "        if preds[j] < cut and labels[j] > cut:\n",
    "            # ax.plot([x[edges[0,j]], x[edges[1,j]]], [y[edges[0,j]], y[edges[1,j]]], '--', c='b')\n",
    "            ax.plot([ptx1, ptx2], [pty1, pty2], '--', color='b', lw=1.5, alpha=0.9)\n",
    "\n",
    "        # False Positives\n",
    "        if preds[j] > cut and labels[j] < cut:\n",
    "            # ax.plot([x[edges[0,j]], x[edges[1,j]]], [y[edges[0,j]], y[edges[1,j]]], '-', c='r', alpha=preds[j])\n",
    "            ax.plot([ptx1, ptx2], [pty1, pty2], '-', color='r', lw=1.5, alpha=0.15)\n",
    "\n",
    "        # True Positives\n",
    "        if preds[j] > cut and labels[j] > cut:\n",
    "            # ax.plot([x[edges[0,j]], x[edges[1,j]]], [y[edges[0,j]], y[edges[1,j]]], '-', c='k', alpha=preds[j])\n",
    "            ax.plot([ptx1, ptx2], [pty1, pty2], '-', color='k', lw=1.5, alpha=0.3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"ambiguous1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_sample_xy(graph, cut=0.7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sample_xy(graph, lower_cut=0.6, upper_cut=0.8, figsize=(15,15)):\n",
    "    \"\"\"\"Draw Sample with True and False Edges\"\"\"\n",
    "    \n",
    "    # coordinate transformation\n",
    "    x = graph.x.detach().numpy()\n",
    "    truth = graph.y_pid\n",
    "    scores = graph.scores[:truth.size(0)]\n",
    "    edges = graph.edge_index\n",
    "    eid = int(graph.event_file[-5:])\n",
    "    pids = np.unique(graph.pid)\n",
    "    \n",
    "    preds, labels = scores.numpy(), truth.numpy()\n",
    "    \n",
    "    \n",
    "    # detector layout\n",
    "    fig, ax = detector_layout(figsize=figsize)\n",
    "    \n",
    "    # True Event\n",
    "    r, phi, ir = x.T\n",
    "    x, y = polar_to_cartesian(r, phi)\n",
    "    \n",
    "    for pid in pids:\n",
    "        idx = graph.pid == pid\n",
    "        ax.plot(x[idx], y[idx], '-', linewidth=1.5)\n",
    "        ax.scatter(x[idx], y[idx], label='particle_id: {}'.format(int(pid)))\n",
    "    \n",
    "    \n",
    "    # Draw the segments\n",
    "    for j in range(labels.shape[0]):\n",
    "        \n",
    "        ptx1 = x[edges[0,j]]\n",
    "        ptx2 = x[edges[1,j]]\n",
    "        pty1 = y[edges[0,j]]\n",
    "        pty2 = y[edges[1,j]]\n",
    "        \n",
    "        # False Negatives\n",
    "        if preds[j] < lower_cut and labels[j] > upper_cut:\n",
    "            # ax.plot([x[edges[0,j]], x[edges[1,j]]], [y[edges[0,j]], y[edges[1,j]]], '--', c='b')\n",
    "            ax.plot([ptx1, ptx2], [pty1, pty2], '--', color='b', lw=1.5, alpha=0.9)\n",
    "\n",
    "        # False Positives\n",
    "        if preds[j] > upper_cut and labels[j] < lower_cut:\n",
    "            # ax.plot([x[edges[0,j]], x[edges[1,j]]], [y[edges[0,j]], y[edges[1,j]]], '-', c='r', alpha=preds[j])\n",
    "            ax.plot([ptx1, ptx2], [pty1, pty2], '-', color='r', lw=1.5, alpha=0.15)\n",
    "\n",
    "        # True Positives\n",
    "        if preds[j] > lower_cut and labels[j] > upper_cut:\n",
    "            # ax.plot([x[edges[0,j]], x[edges[1,j]]], [y[edges[0,j]], y[edges[1,j]]], '-', c='k', alpha=preds[j])\n",
    "            ax.plot([ptx1, ptx2], [pty1, pty2], '-', color='k', lw=1.5, alpha=0.3)\n",
    "    \n",
    "    ax.set_title('Azimuthal View of STT, EventID # {}'.format(eid))\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"ambiguous_{}.png\".format(eid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_files = [18,106,109,113,120,122,133,139,147,152,153,158,159,164,1012,1022,1030,1031,1040,1894,1892,1880,1877,1872,1860,1857,1828,1827,1817,1816,1816,1807,1804,1767,1761,\n",
    "                1751,1750,1749,1743,1734,1722,1721\n",
    "               ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for e in filter_files:\n",
    "    # read test events e.g. gnn_processed/test\n",
    "    graph = torch.load(test_files[e], map_location=device)\n",
    "    draw_sample_xy(graph, lower_cut=0.65, upper_cut=0.75);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# extract hit information\n",
    "r, phi, ir = graph.x.T\n",
    "ir = ir.detach().numpy()*100\n",
    "x, y = polar_to_cartesian(r.detach().numpy(), phi.detach().numpy())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# plot true event\n",
    "fig, ax = detector_layout(figsize=(10,10))\n",
    "pids = np.unique(graph.pid)\n",
    "\n",
    "# True Event\n",
    "for pid in pids:\n",
    "    idx = graph.pid == pid\n",
    "    ax.plot(x[idx], y[idx], '-', linewidth=1.5)\n",
    "    ax.scatter(x[idx], y[idx], label='particle_id: {}'.format(int(pid)))\n",
    "\n",
    "ax.set_title('Azimuthal View of STT, EventID # {}'.format(eid))\n",
    "ax.legend(fontsize=10, loc='best')\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"true_track.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
